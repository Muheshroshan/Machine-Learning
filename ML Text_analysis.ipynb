{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aN7srxZwFE0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "158dbb51-4c33-4e7b-cf10-b3c953bdbff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re, nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading dataset as dataframe\n",
        "df = pd.read_csv('/content/drive/MyDrive/Machine Lerning/CA2_Task_2/Poem_Data.csv')\n",
        "\n",
        "pd.set_option('display.max_colwidth', None) # Setting this so we can see the full content of cells\n",
        "pd.set_option('display.max_columns', None) # to make sure we can see all the columns in output window\n",
        "print(df)\n",
        "# Cleaning Tweets\n",
        "def cleaner(tweet):\n",
        "    soup = BeautifulSoup(tweet, 'lxml') # removing HTML entities such as ‘&amp’,’&quot’,'&gt'; lxml is the html parser and shoulp be installed using 'pip install lxml'\n",
        "    souped = soup.get_text()\n",
        "    re1 = re.sub(r\"(@|http://|https://|www|\\\\x)\\S*\", \" \", souped) # substituting @mentions, urls, etc with whitespace\n",
        "    re2 = re.sub(\"[^A-Za-z]+\",\" \", re1) # substituting any non-alphabetic character that repeats one or more times with whitespace\n",
        "\n",
        "    tokens = nltk.word_tokenize(re2)\n",
        "    lower_case = [t.lower() for t in tokens]\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
        "\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [wordnet_lemmatizer.lemmatize(t,'v') for t in filtered_result]\n",
        "    return lemmas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW5pOsTtG78x",
        "outputId": "f58ec2b4-9c8a-4b80-83d2-0f80ae38563a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Genre  \\\n",
            "0          Music   \n",
            "1          Music   \n",
            "2          Music   \n",
            "3          Music   \n",
            "4          Music   \n",
            "..           ...   \n",
            "836  Environment   \n",
            "837  Environment   \n",
            "838  Environment   \n",
            "839  Environment   \n",
            "840  Environment   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                            Poem  \n",
            "0                                                                                                                                                                                                                                                                                                                                                            NaN  \n",
            "1                                                                                                                                                                In the thick brushthey spend the hottest part of the day,              soaking their hoovesin the trickle of mountain water              the ravine hoardson behalf of the oleander.             \n",
            "2                                                                                                                                                                   Storms are generous.                                      Something so easy to surrender to, sitting by the window, and then you step out into the garden you were so bored of,               \n",
            "3                                                                                                              —After Ana Mendieta Did you carry around the matin star? Did you hold forest-fire in one hand? Would you wake to radiate, shimmer, gleam lucero-light? Through the morning would you measure the wingspan of an idea taking off— & by night would  \n",
            "4                                                                                                                                               for Aja Sherrard at 20The portent may itself be memory. —Wallace StevensHow hard to carry scores of adults on your back,not look at them as carrions of need, the distressof what loyalty requires. This pain is  \n",
            "..                                                                                                                                                                                                                                                                                                                                                           ...  \n",
            "836                                           Why make so much of fragmentary blue In here and there a bird, or butterfly, Or flower, or wearing-stone, or open eye, When heaven presents in sheets the solid hue?Since earth is earth, perhaps, not heaven (as yet)— Though some savants make earth include the sky; And blue so far above us comes so high, It  \n",
            "837                                                                                                Woman, I wish I didn't know your name. What could you be? Silence in my house& the front yard where the dogwoodwouldn't make up its mind about flowers.Aren't you Nature? A stem cringing, half- shadowed beneath a torque of rain.I too am leaving. I too am  \n",
            "838                                                                                                                Yonder to the kiosk, beside the creek, Paddle the swift caque. Thou brawny oarsman with the sunburnt cheek, Quick! for it soothes my heart to hear the Bulbul speak.Ferry me quickly to the Asian shores, Swift bending to your oars. Beneath  \n",
            "839                                                          You come to fetch me from my work to-night When supper's on the table, and we'll see If I can leave off burying the white Soft petals fallen from the apple tree. (Soft petals, yes, but not so barren quite, Mingled with these, smooth bean and wrinkled pea;) And go along with you ere you lose  \n",
            "840  You see them through water and glass, (both liquids) and through air with plenty of liquid in it —water is moving through the air— you see the large dolphins animated, unfractious in their nativedrink, going back and forth interacting with some sort of rings—in a minute-long video— in a loop, we see these dolphins again and again looping through  \n",
            "\n",
            "[841 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert float values to empty string in the 'Poem' column\n",
        "df['Poem'] = df['Poem'].astype(str)\n",
        "df['cleaned_tweet'] = df.Poem.apply(cleaner)\n",
        "\n",
        "df = df[df['cleaned_tweet'].map(len) > 0] # removing rows with cleaned tweets of length 0\n",
        "\n",
        "print(\"Printing top 5 rows of dataframe showing original and cleaned tweets....\")\n",
        "\n",
        "print(df[['Poem','cleaned_tweet']].head())\n",
        "\n",
        "df.drop(['Poem'], axis=1, inplace=True)\n",
        "\n",
        "# Saving cleaned tweets to csv\n",
        "df.to_csv('cleaned_data.csv', index=False)\n",
        "\n",
        "# joining tokens to create strings. TfidfVectorizer does not accept tokens as input\n",
        "df['cleaned_tweet'] = [\" \".join(row) for row in df['cleaned_tweet'].values]\n",
        "\n",
        "data = df['cleaned_tweet']\n",
        "\n",
        "Y = df['Genre'] # target column\n",
        "\n",
        "# min_df=.00015 means that each ngram (unigram, bigram, & trigram) must be present in at least 30 documents for it to be considered as a token (200000*.00015=30). This is a clever way of feature engineering\n",
        "tfidf = TfidfVectorizer(min_df=.00015, ngram_range=(1,3))\n",
        "\n",
        "tfidf.fit(data) # learn vocabulary of entire data\n",
        "\n",
        "data_tfidf = tfidf.transform(data) # creating tfidf values\n",
        "\n",
        "pd.DataFrame(pd.Series(tfidf.get_feature_names_out())).to_csv('vocabulary.csv', header=False, index=False)\n",
        "\n",
        "print(\"Shape of tfidf matrix: \", data_tfidf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SilvbpLbHLTP",
        "outputId": "cbc6dc19-fc3e-43e3-d453-154c69fd4b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-019c22f9f9a8>:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(tweet, 'lxml') # removing HTML entities such as ‘&amp’,’&quot’,'&gt'; lxml is the html parser and shoulp be installed using 'pip install lxml'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing top 5 rows of dataframe showing original and cleaned tweets....\n",
            "                                                                                                                                                                                                                                                 Poem  \\\n",
            "0                                                                                                                                                                                                                                                 nan   \n",
            "1                                                     In the thick brushthey spend the hottest part of the day,              soaking their hoovesin the trickle of mountain water              the ravine hoardson behalf of the oleander.              \n",
            "2                                                        Storms are generous.                                      Something so easy to surrender to, sitting by the window, and then you step out into the garden you were so bored of,                \n",
            "3   —After Ana Mendieta Did you carry around the matin star? Did you hold forest-fire in one hand? Would you wake to radiate, shimmer, gleam lucero-light? Through the morning would you measure the wingspan of an idea taking off— & by night would   \n",
            "4                                    for Aja Sherrard at 20The portent may itself be memory. —Wallace StevensHow hard to carry scores of adults on your back,not look at them as carrions of need, the distressof what loyalty requires. This pain is   \n",
            "\n",
            "                                                                                                                                                                                  cleaned_tweet  \n",
            "0                                                                                                                                                                                         [nan]  \n",
            "1                                                                   [thick, brushthey, spend, hottest, part, day, soak, hoovesin, trickle, mountain, water, ravine, hoardson, behalf, oleander]  \n",
            "2                                                                                                                [storm, generous, something, easy, surrender, sit, window, step, garden, bore]  \n",
            "3  [ana, mendieta, carry, around, matin, star, hold, forest, fire, one, hand, would, wake, radiate, shimmer, gleam, lucero, light, morning, would, measure, wingspan, idea, take, night, would]  \n",
            "4                                        [aja, sherrard, portent, may, memory, wallace, stevenshow, hard, carry, score, adults, back, look, carrions, need, distressof, loyalty, require, pain]  \n",
            "Shape of tfidf matrix:  (841, 44846)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing Support Vector Classifier\n",
        "model = LinearSVC() # kernel = 'linear' and C = 1\n"
      ],
      "metadata": {
        "id": "u0dAQSJLH52x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running cross-validation\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation\n",
        "scores=[]\n",
        "iteration = 0\n",
        "for train_index, test_index in kf.split(data_tfidf, Y):\n",
        "    iteration += 1\n",
        "    print(\"Iteration \", iteration)\n",
        "    X_train, Y_train = data_tfidf[train_index], Y[train_index]\n",
        "    X_test, Y_test = data_tfidf[test_index], Y[test_index]\n",
        "    model.fit(X_train, Y_train) # Fitting SVC\n",
        "    Y_pred = model.predict(X_test)\n",
        "    score = metrics.accuracy_score(Y_test, Y_pred) # Calculating accuracy\n",
        "    print(\"Cross-validation accuracy: \", score)\n",
        "    scores.append(score) # appending cross-validation accuracy for each iteration\n",
        "mean_accuracy = np.mean(scores)\n",
        "print(\"Mean cross-validation accuracy: \", mean_accuracy)"
      ],
      "metadata": {
        "id": "BJ0zXWQ5KGvt",
        "outputId": "9c6a6c5a-2c5d-401f-ed5a-781e0acfa3d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  1\n",
            "Cross-validation accuracy:  0.4823529411764706\n",
            "Iteration  2\n",
            "Cross-validation accuracy:  0.40476190476190477\n",
            "Iteration  3\n",
            "Cross-validation accuracy:  0.4523809523809524\n",
            "Iteration  4\n",
            "Cross-validation accuracy:  0.36904761904761907\n",
            "Iteration  5\n",
            "Cross-validation accuracy:  0.39285714285714285\n",
            "Iteration  6\n",
            "Cross-validation accuracy:  0.5238095238095238\n",
            "Iteration  7\n",
            "Cross-validation accuracy:  0.44047619047619047\n",
            "Iteration  8\n",
            "Cross-validation accuracy:  0.4166666666666667\n",
            "Iteration  9\n",
            "Cross-validation accuracy:  0.5238095238095238\n",
            "Iteration  10\n",
            "Cross-validation accuracy:  0.4523809523809524\n",
            "Mean cross-validation accuracy:  0.4458543417366947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aPKLQhamKI0z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}